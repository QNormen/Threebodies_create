# Threebodies_create
训练一个中文GPT2模型，使用BERT的Tokenizer.中文语料采用三体三部曲。训练10个周期，batchsize=8。最终可以续写10句以上的三体。

1.Description:
---
本工作参考：https://github.com/lvfinn/chinese-GPT2-start-from-zero 仿照该工作整理数据集并训练，实现基础语句生成。
训练一个中文GPT2模型，使用BERT的Tokenizer.中文语料采用三体三部曲。训练10个周期，batchsize=8。最终可以续写10句以上的三体。

2.Start:
----
(1)***environment***

首先，我们下载依赖。
```bash
pip install -r requirements.txt
```

(2)***dataset***

准备中文语料，放置在./threebodys文件夹下，将语料由.txt文件更改为input.json文件
按照参考样例./train.json更改input.json文件格式,由于数据集内容为原始的小说内容，包含着大量的非法字符和json读取不支持的控制字符，因此我们对原始数据集文件进行处理，去除其中非法字符，生成预处理好的数据集文件train.json。
```bash
python create_threebodies.py
python clr_ctrl.py
```

(3)***Model***

在model_config 定义初始GPT-2模型的超参数配置，
- "initializer_range": 0.02 ： 定义了模型参数（如权重矩阵）在初始化时的标准差，权重会在均值为0，标准差为0.02的正态分布中进行随机初始化。
- "layer_norm_epsilon": 1e-05 ： 用于层归一化的常数，用于避免在归一化过程中出现除以零的情况。设置值为1e-05，用于稳定训练。
- "n_ctx": 1024 ： 表示模型上下文窗口的大小，GPT-2 在生成文本时会考虑的最大序列长度。最大长度设为1024，即模型一次最多能处理1024个 token。
- "n_embd": 768 ： 表示每个token的嵌入维度大小，即模型中词向量的维度。设置为768，即每个词汇的表示向量是768维的。
- "n_head": 12 ： 表示自注意力机制中的注意力头的数量。设置为12，即模型的多头注意力机制中有12个独立的头。
- "n_layer": 10 ： 表示 Transformer 编码器中的层数。在这里，设置为 12，即模型有 12 层堆叠的 Transformer 块。
- "n_positions": 1024 ： 表示模型可以处理的最大位置索引，即序列中的最大位置数。最大位置数为 1024，和 n_ctx一致，表示模型最多能处理1024个位置的token。
- "vocab_size": 13317 ： 表示词汇表的大小，即模型可以识别和生成的词汇数量。在这里，词汇表大小为 21128，表示该模型可以处理的词汇量为21128个不同的 token。


(4)***Training***

现在，我们可以使用我们处理好的数据集来训练我们的初始gpt2模型，使用如下命令：
```bash
python train.py   --model_config config/model_config.json   --tokenized_data_path data/tokenized/   --tokenizer_path cache/vocab.txt   --raw_data_path data/train.json   --epochs 10   --log_step 100   --stride 256   --output_dir model/   --device 0,1   --num_pieces 100   --raw
```

训练过程中，每个epoch对应的模型都将存储在./model/目录下，最终训练好的模型将存储在./model/final_model/路径中。

(5)***Generate***

现在，我们可以使用我们用目标语料训练生成的模型来进行文字生成，使用如下命令：
```bash
python generate.py   --device 1   --length 300   --tokenizer_path cache/vocab.txt   --model_path model/final_model   --prefix "[CLS]罗辑大喝一声"   --topp 1   --temperature 1.0 --save_samples --save_samples_path ./mnt/
```

3.Result
--
最终会生成几个文字样本，存储在./mnt/目录下，如下： 

======================================== SAMPLE 1 ========================================
罗辑大喝一声，好半天才知道，这是一场梦见到过的最美好时刻了。现在，那时他真是梦见到的最后一次了。

天深夜，雨停了，在小雨的最后雨发上了下来。初升起来时，很慢，玫瑰星云下起落下，然后消失。雨后来雨了，玫瑰在雨之海中回忆般的晚霞，梦界又在云海中回放回忆梦中的那一缕缕缕雾中最醒的星云，它来自之夜没有云和溶化，但在那个玫瑰色的霓虹中，仿佛是一只永恒的生光;蓝色的深处，梦般的梦般地鸣变得清澈起伏。但有时，雨中的云，玫瑰星云没有看到玫瑰色的，在象离的梦中，它们是永恒的象征。

至于静止，梦般的梦般在以前都是。

又往事了，这天深夜里，华华和小梦见到她抱着自己的梦。她握手中的那个小瓶fg为晶花，这时她只是一眨眼，两两

==========================================================================================

======================================== SAMPLE 2 ========================================
罗辑大喝一声，才想起那个酒球状闪电用的时侯。
但罗辑不想跳出来了，他对那酒瓶是一小酒喝完酒，当然不再喝进他小口酒时，直勾勾地喝着，"你喝一小瓶酒，你到那酒精的时候一下不下。
他们酒喝完酒，直看了天立在酒瓶时，开始点点来灌啤酒，开始喝着那不可乐得大口居然喝下，但看到了，"瓶酒使它喝进去。

"这不正在作了。
“不，用工作了这一个酒，在那个房中，那瓶可能喝着天喝，但要天喝着那瓶。"唉，你可能喝了起来你了，你到了时间了..你知道我了，那瓶酒，我会喝完了下来，你喝着干了起来。你见了起来。
“没关心，这一下来我要靠着。"
"你以为他的大该知道的了，但你知道吧，你知道，医院长是怎么回事了？"
"见过了，老弟，去喝酒，

==========================================================================================

======================================== SAMPLE 3 ========================================
罗辑大喝一声，他们喝得很快又钻了过去。
他们来到一堆堆牛奶后，就吃了起大大杯，并不停地喝了起来，很快把它放到一堆牛奶饮而降。时间不知是没喝，就吃的是没有用，只好吃的酒精在那儿，酒劲把那瓶还给他喝完。

“没吃的，我要喝，酒在这酒也没有用。”

“不要用，你们那瓶在喝，喝完我急了，你们要喝着不那瓶又喝着。”李白卡佳士兵们地喝着喝着酒，放下酒桶里放到了下午地下去。

“到了，不要用啊，不要用了，不着没用的大的大劲种水喝完。"

"你们那你们把那喝着急了？你们当小瓶在那儿喝到上喝着。"瓶在那里，我们喝着发软管儿喝着的胡子来，它端着天立下来，不喝着急剧灌和起来，只好吃，赶紧用一股微微的嗓子喝着。

"你们把这些。

==========================================================================================

======================================== SAMPLE 4 ========================================

罗辑大喝一声，但很快发现，在他们面前，这次只是一个护士的小麻烦地玩了，忘记了他对他说的话。
接着，罗辑说：“我要是刚才对面壁者的一切都是很无所知。”
“那么，罗辑博士，你是谁？”史强问，“我就要轻易争，恢复一个人，就立刻叫大史。”“国家可能有各国家政治。”“博士，你们委员会和军起走了吗？”“您不用，你们一起见鬼地多少巧克了吗？”“时间没有那么多。”“国家。”“为什么？”“一方面的日常联合国有很多人呢？”“那次不对当然。”“国家？”“一百二百年前就设定。”“你不能干什么过那么多年了？你看到过一部长时间，按照法律的法律，法律上不能挑战吗？”“我想当然，如果你指的那些计划是一下吧，后能挑到沙第二个法国上去，那你

==========================================================================================

